---
title: 'Wrapping up: analyzing GSP flight data using Stan and a familiar R syntax'
author: John Johnson
date: '2018-05-08'
categories:
  - Greenville
tags:
  - Bayesian
  - flights
  - Stan
slug: wrapping-up-analyzing-gsp-flight-data-using-stan-and-a-familiar-r-syntax
draft: true
---



<div id="purpose" class="section level1">
<h1>Purpose</h1>
<p>In previous posts we started with a public dataset of departures from my local airport, Greenville-Spartanburg International Airport (GSP) and built up a model using the <a href="http://mc-stan.org">Stan</a> package. This model started out easy enough - a constant plus some random noise, but pretty soon we were able to build up a pretty sophisticated model that took advantage of a number of structural features in the data. The dark side of this approach is that pretty soon we bump into issues affecting numerical computation. In the first case, we were able to solve the issue by reparameterizing into a form that ended up being more natural for the problem anyway. In the second case, which we didn’t really cover a lot, came from the fact that we were using what was really a generalized linear model (i.e. we were modeling negative binomial count data). Sometimes you really have to mess around with the options. Fortunately for a very large class of models, someone has messed around with the options (and other tricks) for us and presented in a package that uses the formula syntax for R users. Here we repeat the negative binomial model using the <code>rstanarm</code> package.</p>
<!--more-->
</div>
<div id="setup-and-data" class="section level1">
<h1>Setup and data</h1>
<p>We preload the data as before, and this time include a new R package:</p>
<pre class="r"><code>library(rstanarm)</code></pre>
<p>With <code>rstanarm</code> we don’t really need to create a separate <code>.stan</code> file; we can just analyze using R code. The work with Stan occurs in the background:</p>
<pre class="r"><code>fit &lt;- stan_glm.nb(n ~ factor(quarter(date)) + factor(wday(date)), data=counts_depart,prior=normal(0,5),prior_intercept=normal(log(32),5))
fit</code></pre>
<pre><code>## stan_glm.nb
##  family:       neg_binomial_2 [log]
##  formula:      n ~ factor(quarter(date)) + factor(wday(date))
##  observations: 335
##  predictors:   10
## ------
##                        Median MAD_SD
## (Intercept)             2.6    0.0  
## factor(quarter(date))2  0.4    0.0  
## factor(quarter(date))3  0.3    0.0  
## factor(quarter(date))4  0.2    0.0  
## factor(wday(date))2     0.1    0.1  
## factor(wday(date))3     0.1    0.1  
## factor(wday(date))4     0.1    0.1  
## factor(wday(date))5     0.1    0.1  
## factor(wday(date))6     0.1    0.1  
## factor(wday(date))7    -0.3    0.1  
## reciprocal_dispersion  39.9    4.7  
## 
## Sample avg. posterior predictive distribution of y:
##          Median MAD_SD
## mean_PPD 17.3    0.4  
## 
## ------
## For info on the priors used see help(&#39;prior_summary.stanreg&#39;).</code></pre>
<p>That is a lot of data manipulation and analysis fit onto one line! Here, instead of explicitly calculating the quarter and the weekday, we included it in the formula part of the <code>stan_glm.nb</code> line. The minor drawback is that the presentation of it looks a little funny, and we lost a little control over the reference class. Also notice this is on the log scale, as the parameters were in the last post. However, we can still review the relative effect sizes. We also specified priors as follows:</p>
<ul>
<li>Intercept prior: distribution with mean equal to our original guess (32, on the log scale) and a standard deviation high enough to cover all reasonable values (again, on the log scale)</li>
<li>Priors for others: conservative, centered at no effect but with high enough standard deviation to cover all reasonable values</li>
</ul>
<p>Now we can plot the results:</p>
<pre class="r"><code>plot(fit,regex_pars=&quot;date&quot;)</code></pre>
<p><img src="/post/2018-05-08-wrapping-up-analyzing-gsp-flight-data-using-stan-and-a-familiar-r-syntax_files/figure-html/unnamed-chunk-3-1.png" width="672" /></p>
<p>The <code>regex_pars</code> option in the above prevented the plotting of the <code>Intercept</code> and <code>reciprocal_dispersion</code>, which would have thrown off the scaling of the above graph. (The large value of the <code>reciprocal_dispersion</code> relative to the square of the intercept means the residual departure from a Poisson count model isn’t too bad; in fact, we might have gotten reasonable results from a Poisson model.)</p>
<p>These results are similar to the ones we observed before - high number of departures in Q2, low in Q1, highest number of departures on Wednesdays (perhaps) and fewest on Sundays. Helpfully, the output gives the mean posterior predictive distribution of the number of departures of about 17 (a far cry from the 32 we guessed way back when, and more in line with results from other methods). As it turns out, we got these insights with an “incorrect” model (one that modeled number of flights as continuous), and the trouble we went through to model these counts correctly gave the same insights.</p>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>There’s a lot that can be done with these Stan models that I glossed over for sake of brevity: traceplots, diagnostics, predictive probabilities, etc. And there’s also that nagging question of “why not just start with <code>rstanarm</code> anyway?” While linear and generalized linear models can shed insight on a lot of things we observe in the world, sometimes it’s better to go through the trouble of building these things up structurally. Sometimes science will suggest an informative prior or a particular relationship between observed phenomena. Stan enables us to model those directly (ok, well, I haven’t modeled any of the crazy relationships in quantum mechanics). But if it turns out a linear or generalized linear model (or linear/generalized linear mixed model, such as repeated measures or hierarchical linear model) is the route to take, and you need to use a Bayesian method (which I almost always prefer), <code>rstanarm</code> is very useful because it cuts down on data manipulation, uses a familiar R formula syntax, and hides a lot of the nasty numerical details (like reparameterizing to avoid divergent transitions in the underlying algorithm) so you can focus on the problem at hand.</p>
<p>This also ends my first series on Stan. There is a small, but growing number of blog posts, books, and other discussions on this excellent piece of software, and the user manual and other documentation at the website is excellent as well. There are a couple of not so introductory examples I’m considering for the next series, or I might go into prediction as I consider machine learning vs. Bayesian learning. I’m not sure yet.</p>
</div>
