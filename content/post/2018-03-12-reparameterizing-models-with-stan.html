---
title: Reparameterizing models with Stan
author: John Johnson
date: '2018-03-12'
slug: reparameterizing-models-with-stan
categories:
  - Greenville
tags:
  - Bayesian
  - flights
  - Stan
draft: false
---



<div id="purpose" class="section level1">
<h1>Purpose</h1>
<p>In the previous <a href="/post/2018-02-19-more-complex-models-for-analyzing-flight-data">post</a>, we took advantage of the rich structure of the Greenville flight data, especially the notions that the number of flights could depend on the weekday and the season of the flights. However, when we put the two together in what seemed like a natural way, we noticed that we started getting convergence problems, and if you looked carefully at the output, you would have noticed something very strange about the weekday estimates - they were all negative! As it turns out, this model is overdetermined - there are too many parameters. Thus, there could be an infinite number of solutions to this problem. I had glossed over this idea when I added only quarters to the model, but adding both we have to face it head on. We do that here.</p>
<!--more-->
</div>
<div id="setup-and-data" class="section level1">
<h1>Setup and data</h1>
<pre class="r"><code>library(tidyverse)
library(lubridate)
library(rstan)
library(bayesplot)
load(&quot;airline_data.RData&quot;)
airline_data %&gt;% 
  mutate(date=ymd(YEAR*10000+MONTH*100+DAY_OF_MONTH),
         wnum = floor((date - ymd(YEAR*10000+0101))/7)) -&gt; 
  airline_data

airline_data %&gt;% 
  filter(ORIGIN_AIRPORT_ID==11996) %&gt;% 
  count(date) -&gt; 
  counts_depart</code></pre>
<pre class="r"><code>options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)</code></pre>
</div>
<div id="reparameterizing" class="section level1">
<h1>Reparameterizing</h1>
<p>Right now we have a model that looks like the following:</p>
<p><span class="math display">\[flights = q_i + w_j + \epsilon \left(i\in \left\{1,2,3,4\right\}, j\in\left\{1,2,3,4,5,6,7\right\}\right)\]</span>,</p>
<p>where <span class="math inline">\(\epsilon \sim \mathscr{N}\left(0,\sigma^2\right)\)</span>. This has an infinite number of solutions. For example, we can find a solution, then subtract 100 from all the <span class="math inline">\(w_j\)</span> and add 100 to the <span class="math inline">\(q_i\)</span> and still have a valid solution. Stan found one of them, but it started looking like there was a bit of trouble, and having all the <span class="math inline">\(w_j\)</span> be negative doesn’t seem to be helpful for interpretation. Instead, let’s pick a model that will have a single solution. First, let’s go back to an overall mean concept. Then we will add in constraints that the sum of the quarter effects is 0, and as well the sum of the weekday effects is 0. This then should be completely determined and have a unique solution (we call this “identifiable” in the statistical modeling world). In math symbols, it looks like this:</p>
<p><span class="math display">\[flights = \mu + q_i + w_j + \epsilon \left(i\in \left\{1,2,3,4\right\}, j\in\left\{1,2,3,4,5,6,7\right\},\sum_{i=1}^4q_i=0,\sum_{j=1}^7w_j=0\right)\]</span></p>
<p>Let’s see what this looks like in Stan:</p>
<pre><code>data {
  int ndates;
  vector[ndates] flights;
  int qtr[ndates];
  int dow[ndates];
}

parameters {
  real mu;
  real&lt;lower=0&gt; sigma;
  vector[3] qtr_effect_raw;
  vector[6] dow_effect_raw;
}

transformed parameters {
  vector[4] qtr_effect;
  for (i in 1:3)
    qtr_effect[i] = qtr_effect_raw[i];
  qtr_effect[4] = -sum(qtr_effect);
  
  vector[7] dow_effect;
  for (j in 1:6)
    dow_effect[j] = dow_effect_raw[j];
  dow_effect[7] = -sum(dow_effect);
}

model {
  flights ~ normal(mu + qtr_effect[qtr] + dow_effect[dow],sigma);
  sigma ~ uniform(0,20);
  mu ~ normal(32,10);
  for (i in 1:3)
    qtr_effect_raw[i] ~ normal(0,10);
  for (j in 1:6)
    dow_effect_raw[j] ~ normal(0,10);
}</code></pre>
<p>Here, we use the <code>transformed parameters</code> block to enforce our idea that the sums of the quarter effects and the day of week effects are 0. If you look at what we did in that block, we basically set <span class="math inline">\(q_4 = -(q_1 + q_2 + q_3)\)</span> and similarly for <span class="math inline">\(w_7\)</span>.</p>
<p>Now, how did I come up with this bit of wizardry? Stan is supposed to be be intuitive! Well, much of it is. And the use of <code>transformed parameters</code> states very clearly what is going on. How I came up with this bit of sorcery is by reading the <a href="http://mc-stan.org/users/documentation/index.html">Stan documentation</a>, specifically Page 135 on identifiabilty.</p>
<p>The lovely part of this? The R code is the same as before, with the exception that I’m changing the variable name of the fit object.</p>
<pre class="r"><code>ndates &lt;- nrow(counts_depart)
flights &lt;- counts_depart$n
qtr &lt;- quarter(counts_depart$date)
dow &lt;- wday(counts_depart$date)

data_to_pass &lt;- c(&quot;ndates&quot;,&quot;flights&quot;,&quot;qtr&quot;,&quot;dow&quot;)
qtr_dow_model_reparm_fit &lt;- stan(&quot;flights_qtr_dow_reparm.stan&quot;,data=data_to_pass)
save(qtr_dow_model_reparm_fit,file=&quot;qtr_dow_model_reparm_fit.RData&quot;)
qtr_dow_model_reparm_fit</code></pre>
<pre><code>## Inference for Stan model: flights_qtr_dow_reparm.
## 4 chains, each with iter=2000; warmup=1000; thin=1; 
## post-warmup draws per chain=1000, total post-warmup draws=4000.
## 
##                      mean se_mean   sd    2.5%     25%     50%     75%
## mu                  17.35    0.00 0.13   17.09   17.26   17.35   17.44
## sigma                2.28    0.00 0.09    2.12    2.22    2.28    2.34
## qtr_effect_raw[1]   -3.52    0.00 0.21   -3.94   -3.66   -3.52   -3.38
## qtr_effect_raw[2]    2.87    0.00 0.22    2.45    2.72    2.87    3.02
## qtr_effect_raw[3]    0.41    0.00 0.24   -0.07    0.25    0.41    0.57
## dow_effect_raw[1]   -0.83    0.00 0.30   -1.42   -1.03   -0.82   -0.62
## dow_effect_raw[2]    0.87    0.00 0.31    0.27    0.66    0.87    1.08
## dow_effect_raw[3]    0.91    0.00 0.31    0.30    0.71    0.92    1.12
## dow_effect_raw[4]    1.45    0.00 0.31    0.84    1.25    1.45    1.66
## dow_effect_raw[5]    1.40    0.00 0.30    0.80    1.19    1.40    1.59
## dow_effect_raw[6]    1.12    0.00 0.31    0.50    0.91    1.12    1.33
## qtr_effect[1]       -3.52    0.00 0.21   -3.94   -3.66   -3.52   -3.38
## qtr_effect[2]        2.87    0.00 0.22    2.45    2.72    2.87    3.02
## qtr_effect[3]        0.41    0.00 0.24   -0.07    0.25    0.41    0.57
## qtr_effect[4]        0.24    0.00 0.21   -0.17    0.09    0.24    0.38
## dow_effect[1]       -0.83    0.00 0.30   -1.42   -1.03   -0.82   -0.62
## dow_effect[2]        0.87    0.00 0.31    0.27    0.66    0.87    1.08
## dow_effect[3]        0.91    0.00 0.31    0.30    0.71    0.92    1.12
## dow_effect[4]        1.45    0.00 0.31    0.84    1.25    1.45    1.66
## dow_effect[5]        1.40    0.00 0.30    0.80    1.19    1.40    1.59
## dow_effect[6]        1.12    0.00 0.31    0.50    0.91    1.12    1.33
## dow_effect[7]       -4.93    0.00 0.31   -5.54   -5.14   -4.93   -4.71
## lp__              -444.21    0.05 2.42 -449.86 -445.64 -443.85 -442.43
##                     97.5% n_eff Rhat
## mu                  17.59  4000    1
## sigma                2.47  4000    1
## qtr_effect_raw[1]   -3.11  4000    1
## qtr_effect_raw[2]    3.30  4000    1
## qtr_effect_raw[3]    0.89  4000    1
## dow_effect_raw[1]   -0.23  4000    1
## dow_effect_raw[2]    1.48  4000    1
## dow_effect_raw[3]    1.53  4000    1
## dow_effect_raw[4]    2.08  4000    1
## dow_effect_raw[5]    2.00  4000    1
## dow_effect_raw[6]    1.72  4000    1
## qtr_effect[1]       -3.11  4000    1
## qtr_effect[2]        3.30  4000    1
## qtr_effect[3]        0.89  4000    1
## qtr_effect[4]        0.66  4000    1
## dow_effect[1]       -0.23  4000    1
## dow_effect[2]        1.48  4000    1
## dow_effect[3]        1.53  4000    1
## dow_effect[4]        2.08  4000    1
## dow_effect[5]        2.00  4000    1
## dow_effect[6]        1.72  4000    1
## dow_effect[7]       -4.33  4000    1
## lp__              -440.53  2084    1
## 
## Samples were drawn using NUTS(diag_e) at Sun Jun 03 12:46:12 2018.
## For each parameter, n_eff is a crude measure of effective sample size,
## and Rhat is the potential scale reduction factor on split chains (at 
## convergence, Rhat=1).</code></pre>
<p>So if you look at the messages from the compiling, you get the following:</p>
<pre><code>DIAGNOSTIC(S) FROM PARSER:
Warning (non-fatal):
Left-hand side of sampling statement (~) may contain a non-linear transform of a parameter or local variable.
If it does, you need to include a target += statement with the log absolute determinant of the Jacobian of the transform.
Left-hand-side of sampling statement:
    qtr_effect[i] ~ normal(...)</code></pre>
<p>Things just got real! Stan was warning us that we would have to start doing things by hand! Fortunately, we can ignore that for now But as you start adding in fancier nonlinear transforms, you will have to start paying attention to this. We’re far away from that in this series.</p>
<p>Let’s look at the convergence diagnostics. All the <code>Rhats</code> are 1, and the <code>n_eff</code>s are close to or equal to 4000. So far so good, and it looks like our reparameterization is working. Let’s look at the traceplot of the quarter effects:</p>
<pre class="r"><code>p &lt;- mcmc_trace(as.array(qtr_dow_model_reparm_fit),pars=&quot;sigma&quot;,regex_pars = &quot;qtr_effect\\[&quot;,
                facet_args = list(nrow = 5, labeller = label_parsed))
p</code></pre>
<p><img src="/post/2018-03-12-reparameterizing-models-with-stan_files/figure-html/traceplot-1.png" width="672" /></p>
<p>Nothing looks concerning here. I leave it up to you to look at the same for the day of week effects. Let’s look at the density plot of the overall effects <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>:</p>
<pre class="r"><code>p &lt;- mcmc_areas(as.matrix(qtr_dow_model_reparm_fit),pars=c(&quot;mu&quot;,&quot;sigma&quot;),prob = 0.5) +
  ggtitle(&quot;Posterior distributions with medians and 50% intervals&quot;)
p</code></pre>
<p><img src="/post/2018-03-12-reparameterizing-models-with-stan_files/figure-html/densityplotmusig-1.png" width="672" /></p>
<p><span class="math inline">\(\mu\)</span> here close to what we had in the <a href="/post/2018-01-15-analyzing-gsp-flights-using-bayesian-analysis-with-stan">mean model</a>. However, <span class="math inline">\(\sigma\)</span> is about half the magnitude, indicating that we have successfully explained a large part of the variance in number of flights using the quarter and day of week.</p>
<p>Now let’s look at the density plots of the quarter effects:</p>
<pre class="r"><code>p &lt;- mcmc_areas(as.matrix(qtr_dow_model_reparm_fit),regex_pars = &quot;qtr_effect\\[&quot;,prob = 0.5) +
  ggtitle(&quot;Posterior distributions with medians and 50% intervals&quot;)
p</code></pre>
<p><img src="/post/2018-03-12-reparameterizing-models-with-stan_files/figure-html/densityplot-1.png" width="672" /></p>
<p>So, we can interpret this plot as Quarter 2 as the busiest quarters, which makes sense because Quarter 2 is the season of summer vacations. Also, in the original line plot, we noticed that the airport has the most departing flights in the summer, and that comes through here. Quarter 1 is by far the slowest.</p>
<p>There was a trick with the <code>regex_pars</code> that I used. I used <code>regex_pars = &quot;qtr_effect\\[&quot;</code> rather than <code>regex_pars = &quot;qtr_effect&quot;</code> because I wanted to exclude the redundant <code>qtr_effect_raw</code> variables. They were just temporary tools that we used to get <code>qtr_effect</code> to behave the way we wanted, but we really don’t want to perform inference on them. R has a few oddities in the way you use regular expressions, including the use of the double backslash <code>\\</code> in certain situations. Here I wanted to force Stan to show only those <code>qtr_effect</code> variables followed directly by the character <code>[</code> indicating an index number. The same trick applies to <code>dow_effect</code> below as well.</p>
<p>Here are the density plots of the weekday effects.</p>
<pre class="r"><code>p &lt;- mcmc_areas(as.matrix(qtr_dow_model_reparm_fit),regex_pars = &quot;dow_effect\\[&quot;,prob = 0.5) +
  ggtitle(&quot;Posterior distributions with medians and 50% intervals&quot;)
p</code></pre>
<p><img src="/post/2018-03-12-reparameterizing-models-with-stan_files/figure-html/densityplot2-1.png" width="672" /></p>
<p>Recall that <code>wday</code> returns 7 on Saturdays. It turns out that Saturdays are the slowest days in terms of departing flights, followed by Sundays. Travel, at least departing from GSP, is higher during the weekdays, with the highest on Wednesdays. An airport administrator might guess this would be due to the fact that business travel occurs mostly during the week, and during the summer and holidays where vacation travel dominates (presumably), families would rather do things other than travel on Saturdays and Sundays.</p>
</div>
<div id="discussion" class="section level1">
<h1>Discussion</h1>
<p>We had our first run-in with model trouble last time, and we fixed it with reparameterization. Reparameterization turns out to be a very important tool in computational Bayesian analysis (i.e. using Stan, BUGS, or even your own hand-coded MCMC routine). Here we fixed a problem with identifiability, and in this simple case that fixed a problem with the algorithm.</p>
<p>It’s time to step back and take stock of what’s happened so far in this series. We had a set of data that looked like it had an interesting pattern. We took the following steps with it:</p>
<ol style="list-style-type: decimal">
<li>Plotted it</li>
<li>Fit a simple model to it using some basic prior estimates from our knowledge</li>
<li>Fit a more complex model because of a pattern we thought we recognized in the data</li>
<li>Fit an even more complex model because of another pattern we thought we recognized in the data</li>
<li>Saw some issues with that model, took a step back, and reparameterized to fix both some conceptual and computation issues</li>
</ol>
<p>The last two steps might repeat many times, and while that is going on it will be helpful to revisit the objectives of the analysis. We’re going to iterate this adding complexity one more time because of one nagging feeling. We modeled the number of flights with a normal distribution because of convenience, but the number of flights is not a continuous variable. We would rather use something that can model count data.</p>
</div>
